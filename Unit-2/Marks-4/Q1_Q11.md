### 1. **Probabilistic Discriminative Models**

* **Definition**: These models estimate the probability of a class given input features, denoted as $P(y|x)$.

* **Focus**: They learn the boundary between classes, aiming to distinguish between them effectively.

* **Use in Classification**: By estimating class probabilities, they assign new inputs to the class with the highest probability.

* **Example**: Logistic Regression predicts whether an email is spam or not by estimating the probability of each class.

---

### 2. **Gradient Descent vs. Stochastic Gradient Descent**

* **Gradient Descent (GD)**:

  * Uses the entire dataset to compute gradients.
  * Converges smoothly but can be slow, especially with large datasets.

* **Stochastic Gradient Descent (SGD)**:

  * Uses one data point at a time to compute gradients.
  * Converges faster but with more fluctuations.

* **Comparison**:

  * GD: Smooth and stable convergence, but computationally expensive.
  * SGD: Faster convergence, but with more variance in updates.

---

### 3. **Gradient Descent vs. Stochastic Gradient Descent (Reiteration)**

* **Gradient Descent**:

  * Computes gradients using the full dataset.
  * Ideal for smaller datasets.([Sebastian Raschka][1])

* **Stochastic Gradient Descent**:

  * Computes gradients using a single data point.
  * Suitable for large datasets or online learning.

* **Key Difference**:

  * GD provides precise updates but is slower.
  * SGD offers faster updates with more variability.

---

### 4. **Bayesian Linear Regression – Role of Prior**

* **Prior Distribution**:

  * Represents beliefs about model parameters before observing data.

* **Effect on Predictions**:

  * Incorporates prior knowledge, influencing the model's behavior.
  * Helps in regularization, preventing overfitting.

* **Example**:

  * If prior belief suggests a parameter should be small, the model will be biased towards smaller values unless data strongly contradicts this.

---

### 5. **Negative $R^2$ in Linear Regression**

* **Definition**:

  * $R^2$ measures the proportion of variance explained by the model.

* **Negative $R^2$**:

  * Occurs when the model performs worse than a horizontal line at the mean of the target variable.([Cross Validated][2])

* **Interpretation**:

  * Indicates a poor fit, possibly due to incorrect model choice or overfitting.

---

### 6. **Least Squares Regression Equation**

* **Equation**:

  * $y = \beta_0 + \beta_1 x + \epsilon$

* **Explanation**:

  * $\beta_0$: Intercept (value of $y$ when $x = 0$).
  * $\beta_1$: Slope (change in $y$ for a unit change in $x$).
  * $\epsilon$: Error term (difference between observed and predicted values).

* **Goal**:

  * Minimize the sum of squared errors to find the best-fitting line.

---

### 7. **Five Widely Used ML Algorithms & Applications**

1. **Linear Regression**:

   * Predicts continuous values (e.g., house prices).

2. **Logistic Regression**:

   * Classifies binary outcomes (e.g., spam vs. not spam).

3. **Decision Trees**:

   * Classifies or regresses by learning simple decision rules.

4. **k-Means Clustering**:

   * Groups similar data points into clusters.

5. **Random Forest**:

   * Ensemble of decision trees for improved accuracy.

---

### 8. **Supervised vs. Unsupervised Learning**

| Aspect      | Supervised Learning                  | Unsupervised Learning              |                          |
| ----------- | ------------------------------------ | ---------------------------------- | ------------------------ |
| **Data**    | Labeled (input-output pairs)         | Unlabeled (only inputs)            |                          |
| **Goal**    | Learn mapping from inputs to outputs | Find patterns or groupings in data |                          |
| **Example** | Email spam detection                 | Customer segmentation              | ([Statistics By Jim][3]) |

---

### 9. **Five Widely Used ML Algorithms & Applications (Reiteration)**

1. **Linear Regression**:

   * Predicts continuous values.

2. **Logistic Regression**:

   * Binary classification.

3. **Decision Trees**:

   * Classification and regression.([PMC][4])

4. **k-Means Clustering**:

   * Unsupervised grouping.

5. **Random Forest**:

   * Ensemble method for classification and regression.

---

### 10. **Simple Causal Network for Disease Symptoms**

* **Nodes**:

  * Disease (e.g., Flu)
  * Symptoms (e.g., Fever, Cough)

* **Arrows**:

  * Disease → Symptoms (indicating causality)

* **Conditional Probabilities**:

  * P(Fever | Flu) = 0.8
  * P(Cough | Flu) = 0.6

* **Use**:

  * Predict symptoms based on disease presence.([Statistics By Jim][3])

---

### 11. **Advantages & Limitations of Causal Networks**

* **Advantages**:

  * Model cause-effect relationships.
  * Aid in decision-making and interventions.
  * Provide explainable insights.([blog.dailydoseofds.com][5])

* **Limitations**:

  * Require expert knowledge for structure.
  * Computationally intensive.
  * Sensitive to incorrect assumptions.

* **Comparison to Statistical Models**:

  * Statistical models identify correlations; causal networks identify causal relationships.
